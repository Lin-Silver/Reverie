#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import json
import torch
import time
import shutil
import logging
import argparse
import traceback
import threading
import subprocess
from pathlib import Path
from transformers import TextStreamer
import requests
from pydub import AudioSegment
from pydub.playback import play
import importlib.util
import platform
from transformers import AutoConfig, CLIPImageProcessor
from typing import Optional
import re
from concurrent.futures import ThreadPoolExecutor
import uuid
import colorama
from colorama import Fore, Back, Style
from transformers import StoppingCriteriaList, StoppingCriteria

# Initialize colorama for cross-platform colored terminal output
colorama.init(autoreset=True)

# è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨pygameæ¬¢è¿æ¶ˆæ¯
os.environ["PYGAME_HIDE_SUPPORT_PROMPT"] = "1"

# æ·»åŠ pygameéŸ³é¢‘æ’­æ”¾åº“
try:
    import pygame
    pygame.mixer.init(frequency=44100, buffer=4096)  # å¢åŠ ç¼“å†²åŒºå¤§å°ï¼Œå‡å°‘IOé—®é¢˜
except ImportError:
    pass

# å¤šæ¶æ„æ¨¡å‹æ”¯æŒ
try:
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
        TextIteratorStreamer
    )
except ImportError:
    pass

try:
    from llama_cpp import Llama
except ImportError:
    pass

# é…ç½®å½©è‰²æ—¥å¿—
class ColoredFormatter(logging.Formatter):
    """è‡ªå®šä¹‰å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    COLORS = {
        'DEBUG': Fore.CYAN,
        'INFO': Fore.GREEN,
        'WARNING': Fore.YELLOW,
        'ERROR': Fore.RED,
        'CRITICAL': Fore.RED + Style.BRIGHT
    }
    
    def format(self, record):
        # å½©è‰²çº§åˆ«åç§°
        level_name = record.levelname
        if level_name in self.COLORS:
            record.levelname = f"{self.COLORS[level_name]}{level_name}{Style.RESET_ALL}"
        
        # å½©è‰²æ—¶é—´æˆ³
        asctime = self.formatTime(record, self.datefmt)
        record.asctime = f"{Fore.BLUE}{asctime}{Style.RESET_ALL}"
        
        # ç‰¹æ®Šæ ‡è®°æŸäº›å…³é”®è¯
        if hasattr(record, 'msg') and isinstance(record.msg, str):
            if "åŠ è½½æˆåŠŸ" in record.msg:
                record.msg = record.msg.replace("åŠ è½½æˆåŠŸ", f"{Fore.GREEN}åŠ è½½æˆåŠŸ{Style.RESET_ALL}")
            elif "å¤±è´¥" in record.msg:
                record.msg = record.msg.replace("å¤±è´¥", f"{Fore.RED}å¤±è´¥{Style.RESET_ALL}")
            elif "é”™è¯¯" in record.msg:
                record.msg = record.msg.replace("é”™è¯¯", f"{Fore.RED}é”™è¯¯{Style.RESET_ALL}")
            elif "æˆåŠŸ" in record.msg:
                record.msg = record.msg.replace("æˆåŠŸ", f"{Fore.GREEN}æˆåŠŸ{Style.RESET_ALL}")
        
        return super().format(record)

# é…ç½®æ—¥å¿—å¤„ç†å™¨
handler = logging.StreamHandler()
handler.setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))

# é…ç½®æ—¥å¿—
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(handler)

# ç§»é™¤é»˜è®¤å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
for hdlr in logger.handlers[:]:
    if isinstance(hdlr, logging.StreamHandler) and hdlr != handler:
        logger.removeHandler(hdlr)

# å®šä¹‰éŸ³é¢‘æ’­æ”¾ä¸´æ—¶ç›®å½• - ä½¿ç”¨åº”ç”¨è‡ªèº«ç›®å½•ä¸‹çš„å›ºå®šç›®å½•
AUDIO_TEMP_DIR = Path("./audio_temp")
if not AUDIO_TEMP_DIR.exists():
    AUDIO_TEMP_DIR.mkdir(parents=True, exist_ok=True)

# å®‰å…¨éŸ³é¢‘ç›®å½• - å›ºå®šä½ç½®
SAFE_AUDIO_DIR = Path("./Audio/Safe_Audio")
if not SAFE_AUDIO_DIR.exists():
    SAFE_AUDIO_DIR.mkdir(parents=True, exist_ok=True)

# ç§»é™¤è‡ªåŠ¨æ¸…ç†ï¼Œæ”¹ä¸ºå•ç‹¬è„šæœ¬
def cleanup_temp_files(max_age_seconds=3600):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹ä¸­çš„æ—§æ–‡ä»¶"""
    try:
        now = time.time()
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        for file_path in AUDIO_TEMP_DIR.glob("*.*"):
            # ä»…å¤„ç†éŸ³é¢‘æ–‡ä»¶
            if file_path.suffix.lower() in ['.wav', '.mp3']:
                file_age = now - file_path.stat().st_mtime
                if file_age > max_age_seconds:
                    try:
                        file_path.unlink()
                        logging.info(f"å·²æ¸…ç†è¿‡æœŸä¸´æ—¶æ–‡ä»¶: {file_path.name}")
                    except Exception as e:
                        logging.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
        
        # ä¿ç•™æœ€æ–°çš„å®‰å…¨éŸ³é¢‘æ–‡ä»¶ï¼Œæ¸…ç†æ—§çš„
        safe_files = list(SAFE_AUDIO_DIR.glob("*.wav"))
        if len(safe_files) > 10:  # ä¿ç•™æœ€æ–°çš„10ä¸ªæ–‡ä»¶
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            safe_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            # åˆ é™¤æ—§æ–‡ä»¶
            for old_file in safe_files[10:]:
                try:
                    old_file.unlink()
                    logging.info(f"å·²æ¸…ç†æ—§çš„å®‰å…¨éŸ³é¢‘æ–‡ä»¶: {old_file.name}")
                except Exception as e:
                    logging.warning(f"æ¸…ç†å®‰å…¨éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {old_file} - {str(e)}")
    except Exception as e:
        logging.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å‡ºé”™: {str(e)}")

def sanitize_path(path):
    """å¤„ç†è·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿Windowsè·¯å¾„æœ‰æ•ˆ"""
    if isinstance(path, str):
        path = Path(path)
    
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    path = path.resolve()
    
    # å¤„ç†Windowsè·¯å¾„é™åˆ¶
    if platform.system() == "Windows":
        # è·¯å¾„é•¿åº¦é™åˆ¶
        if len(str(path)) > 240:
            try:
                # åˆ›å»ºçŸ­è·¯å¾„åˆ«å
                short_path = AUDIO_TEMP_DIR / f"short_{int(time.time())}.wav"
                shutil.copy2(path, short_path)
                return short_path
            except:
                # å¤åˆ¶å¤±è´¥åˆ™ç»§ç»­ä½¿ç”¨åŸè·¯å¾„
                pass
    
    return path

class ReverieAI:
    """Reverie AIæ ¸å¿ƒç±»ï¼ˆæ”¯æŒå¤šæ¶æ„æ¨¡å‹å’Œè‡ªåŠ¨å†…å­˜ç®¡ç†ï¼‰"""
    
    def __init__(self, model_path: str, use_gpu: bool = False, load_in_4bit: bool = False, 
                 use_tts: bool = False, spark_tts: bool = False, prompt_text: str = None,
                 prompt_speech_path: str = None, device: str = "0", save_dir: str = "Audio",
                 model_dir: str = None):
        # åˆå§‹åŒ–å‚æ•°ï¼Œç»Ÿä¸€å°†æ¨¡å‹è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡ï¼Œå¹¶åŒæ—¶è®°å½•å­—ç¬¦ä¸²å½¢å¼çš„ç»å¯¹è·¯å¾„
        self.model_path = Path(model_path)
        self.model_path_str = str(self.model_path.resolve())
        self.use_gpu = use_gpu
        self.use_tts = use_tts
        self.spark_tts = spark_tts
        self.load_in_4bit = load_in_4bit
        self.prompt_text = prompt_text or "é›¨æ—é‡Œå¯ä»¥äº¤ç»™æˆ‘çš„çœ·å±ä»¬ï¼ŒåŸå¸‚é‡Œæˆ‘å°±æ‹œæ‰˜ä¸€äº›å°å­©å­å§ã€‚"
        self.prompt_speech_path = Path(prompt_speech_path or "models/tts/Nahida.wav")
        self.device_id = device
        self.save_dir = Path(save_dir)
        self.model_dir = Path(model_dir) if model_dir else None
        self.cuda_available = torch.cuda.is_available()
        self.device = "cuda" if (use_gpu and self.cuda_available) else "cpu"
        self.idle_timeout = 600  # 10åˆ†é’Ÿç©ºé—²è¶…æ—¶
        
        # å¤šæ¨¡æ€æ”¯æŒ
        self.vision_enabled = False
        self.image_processor = None
        self.processor = None
        self.cache_dir = Path("cache")
        self.supported_image_ext = ['.jpg', '.jpeg', '.png', '.webp']
        
        # çº¿ç¨‹å®‰å…¨
        self.lock = threading.Lock()
        self.model_loaded = False
        self.last_used = time.time()
        self.conversation_history = []
        self.max_history_length = 10  # è®¾ç½®å¯¹è¯å†å²è®°å½•çš„æœ€å¤§é•¿åº¦ï¼ˆè½®æ¬¡æ•°ï¼‰

        # é˜²æ­¢æ­»å¾ªç¯çš„è®¾ç½®
        self.max_generation_time = 2400  # æœ€é•¿ç”Ÿæˆæ—¶é—´(ç§’)ï¼Œå¢åŠ åˆ°4åˆ†é’Ÿ
        self.max_new_tokens = 20480  # æœ€å¤§ç”Ÿæˆtokenæ•°
        self.repetition_window = 32  # ç”¨äºæ£€æµ‹é‡å¤çš„çª—å£å¤§å°
        self.repetition_threshold = 5  # é‡å¤çš„æ¬¡æ•°é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™è®¤ä¸ºæ˜¯æ­»å¾ªç¯

        # ä¼˜åŒ–å™¨é“¾ï¼šç”¨äºåŠ è½½æ¨¡å‹åè‡ªåŠ¨åº”ç”¨ä¸€ç³»åˆ—ä¼˜åŒ–
        self.model_optimizers = []

        # é»˜è®¤æ³¨å†Œä¼˜åŒ–å™¨æ¨¡å—ï¼šæ ¹æ®æ ‡å¿—æ³¨å†Œ4ä½é‡åŒ–å’ŒFlash Attentionä¼˜åŒ–
        if self.load_in_4bit:
            self.register_model_optimizer(lambda x: x._optimize_quant_4bit())
        if self.use_gpu:
            self.register_model_optimizer(lambda x: x._optimize_flash_attention())
        # æ³¨å†ŒBitNetä¼˜åŒ–å™¨ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨ï¼‰
        self.register_model_optimizer(lambda x: x._optimize_bitnet())

        # åˆå§‹åŒ–æµç¨‹
        self._detect_model_type()
        self._detect_vision_support()  # æ£€æµ‹è§†è§‰æ”¯æŒ
        self._start_inactive_timer()
        self._clean_image_cache()
        
        # ä¸å†è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œæ”¹ä¸ºå•ç‹¬çš„clean.pyè„šæœ¬
        # è¯·ä½¿ç”¨ start-clean.bat æˆ–ç›´æ¥è¿è¡Œ python clean.py æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        # GPUä¿¡æ¯æ˜¾ç¤º
        if self.cuda_available and self.use_gpu:
            props = torch.cuda.get_device_properties(0)
            total_gb = props.total_memory / (1024 ** 3)
            logging.info(f"æ£€æµ‹åˆ°GPUï¼Œæ€»æ˜¾å­˜ï¼š{total_gb:.2f}GB")

        logging.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡ç±»å‹: {self.device.upper()}")

        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        self.save_dir.mkdir(parents=True, exist_ok=True)
        # ç¡®ä¿éŸ³é¢‘å®‰å…¨ç›®å½•å­˜åœ¨
        SAFE_AUDIO_DIR.mkdir(parents=True, exist_ok=True)

    def _get_venv_python(self):
        """è·å–è™šæ‹Ÿç¯å¢ƒPythonè·¯å¾„"""
        if platform.system() == "Windows":
            return str(Path("./venv/Scripts/python.exe").resolve())
        else:
            return str(Path("./venv/bin/python").resolve())

    def _detect_vision_support(self):
        """æ£€æµ‹æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰è¾“å…¥"""
        try:
            # é¦–å…ˆå°è¯•åŠ è½½é…ç½®ï¼Œç¡®ä¿è®¾ç½®trust_remote_code=True
            config = AutoConfig.from_pretrained(self.model_path_str, trust_remote_code=True)
            if hasattr(config, 'vision_config') or hasattr(config, 'vision_encoder'):
                self.vision_enabled = True
                logging.info("æ£€æµ‹åˆ°è§†è§‰æ¨¡å‹æ”¯æŒ")
        except Exception as e:
            logging.warning(f"è§†è§‰æ”¯æŒæ£€æµ‹å¤±è´¥: {str(e)}")
            logging.info("å°è¯•ä»¥å…¼å®¹æ¨¡å¼ç»§ç»­åŠ è½½")
            
    def _process_images(self) -> Optional[torch.Tensor]:
        """å¤„ç†ç¼“å­˜ç›®å½•ä¸­çš„å›¾ç‰‡"""
        if not self.cache_dir.exists():
            return None
        
        image_files = sorted([
            f for f in self.cache_dir.iterdir()
            if f.suffix.lower() in self.supported_image_ext
        ])
        
        if not image_files:
            return None
        
        try:
            from PIL import Image
            image = Image.open(image_files[0]).convert("RGB")
            
            if self.processor:
                return self.processor(images=image, return_tensors="pt")["pixel_values"].to(self.device)
            elif self.image_processor:
                return self.image_processor(image, return_tensors="pt")["pixel_values"].to(self.device)
            return None
        except Exception as e:
            logging.error(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
            return None
        
    def _clean_image_cache(self):
        """æ¸…ç†å›¾ç‰‡ç¼“å­˜"""
        if self.cache_dir.exists():
            for f in self.cache_dir.glob("*"):
                try:
                    if f.suffix.lower() in self.supported_image_ext:
                        f.unlink()
                except:
                    pass

    def _detect_model_type(self):
        """æ™ºèƒ½æ£€æµ‹æ¨¡å‹æ¶æ„"""
        try:
            # ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼çš„æ¨¡å‹è·¯å¾„
            model_path_str = self.model_path_str
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯GGUFæ ¼å¼
            if isinstance(self.model_path, Path):
                model_suffix = self.model_path.suffix.lower()
            else:
                model_suffix = os.path.splitext(model_path_str)[1].lower()
                
            if (model_suffix == ".gguf") or (model_suffix == ".ggml"):
                self.model_type = "gguf"
                logging.info("æ£€æµ‹åˆ°GGUFæ¨¡å‹æ ¼å¼")
                return

            # æ£€æŸ¥config.jsonæ–‡ä»¶
            if isinstance(self.model_path, Path):
                config_path = self.model_path / "config.json"
            else:
                config_path = os.path.join(model_path_str, "config.json")
                
            if os.path.exists(config_path):
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        architecture = config.get("architectures", [""])[0].lower() if "architectures" in config else ""
                        model_type = config.get("model_type", "").lower()
                        
                        # æ£€æµ‹BitNetæ¨¡å‹
                        if "bitnet" in architecture or "bitnet" in model_type:
                            self.model_type = "bitnet"
                            logging.info(f"{Fore.CYAN}æ£€æµ‹åˆ°BitNetæ¨¡å‹æ¶æ„: {architecture}{Style.RESET_ALL}")
                            # BitNetæ¨¡å‹å¼ºåˆ¶è®¾ç½®trust_remote_code
                            logging.info(f"{Fore.YELLOW}BitNetæ¨¡å‹éœ€è¦è®¾ç½®trust_remote_code=True{Style.RESET_ALL}")
                            return
                        
                        # æ£€æµ‹æ¨¡å‹ç±»å‹
                        if any(x in architecture for x in ["llama", "mistral", "mixtral"]):
                            self.model_type = "llama"
                            logging.info(f"æ£€æµ‹åˆ°LLAMAæ—æ¨¡å‹æ¶æ„: {architecture}")
                        elif any(x in architecture for x in ["moe", "qwen", "deepseek", "gpt-neox"]):
                            self.model_type = "moe"
                            logging.info(f"æ£€æµ‹åˆ°MOEæ—æ¨¡å‹æ¶æ„: {architecture}")
                        elif "phi" in architecture:
                            self.model_type = "phi"
                            logging.info(f"æ£€æµ‹åˆ°PHIæ—æ¨¡å‹æ¶æ„: {architecture}")
                        elif "gpt" in architecture:
                            self.model_type = "gpt"
                            logging.info(f"æ£€æµ‹åˆ°GPTæ—æ¨¡å‹æ¶æ„: {architecture}")
                        else:
                            self.model_type = "hf"
                            logging.info(f"ä½¿ç”¨é€šç”¨HFæ¶æ„åŠ è½½æ¨¡å‹: {architecture}")
                except Exception as e:
                    logging.warning(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ£€æµ‹: {str(e)}")
                    if "trust_remote_code" in str(e):
                        logging.info(f"{Fore.YELLOW}æ£€æµ‹åˆ°éœ€è¦trust_remote_code=True{Style.RESET_ALL}")
            else:
                # å°è¯•æŸ¥æ‰¾æ¨¡å‹ç›®å½•ç»“æ„æ¥æ¨æ–­ç±»å‹
                if os.path.exists(model_path_str):
                    files = os.listdir(model_path_str)
                    if any("ggml" in f.lower() for f in files):
                        self.model_type = "gguf"
                        logging.info("åŸºäºç›®å½•å†…å®¹ï¼Œæ¨æµ‹ä¸ºGGUFæ¨¡å‹")
                    elif any("pytorch_model" in f.lower() for f in files):
                        self.model_type = "hf"
                        logging.info("åŸºäºç›®å½•å†…å®¹ï¼Œæ¨æµ‹ä¸ºHuggingFaceæ¨¡å‹")
                    else:
                        self.model_type = "hf"  # é»˜è®¤ä½¿ç”¨hfåŠ è½½
                        logging.info("æ— æ³•ç¡®å®šæ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨HuggingFaceåŠ è½½å™¨")
                else:
                    self.model_type = "hf"  # é»˜è®¤ä½¿ç”¨hfåŠ è½½
                    logging.warning(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œé»˜è®¤ä½¿ç”¨HuggingFaceåŠ è½½å™¨: {model_path_str}")
        except Exception as e:
            self.model_type = "hf"  # å‡ºé”™æ—¶é»˜è®¤ä½¿ç”¨hf
            logging.error(f"æ¨¡å‹ç±»å‹æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤HuggingFaceåŠ è½½å™¨: {str(e)}")
            logging.info(f"æ¨¡å‹è·¯å¾„: {self.model_path_str}")
            try:
                if isinstance(self.model_path, Path) and self.model_path.exists():
                    logging.info(f"æ¨¡å‹ç›®å½•å†…å®¹: {list(self.model_path.iterdir())[:5]}")
                elif os.path.exists(str(self.model_path)):
                    logging.info(f"æ¨¡å‹ç›®å½•å†…å®¹: {os.listdir(str(self.model_path))[:5]}")
            except:
                pass

    def _load_model(self):
        """å®‰å…¨åŠ è½½æ¨¡å‹"""
        with self.lock:
            if self.model_loaded:
                return

            logging.info(f"æ­£åœ¨åŠ è½½ {self.model_type.upper()} æ¨¡å‹...")
            try:
                if self.model_type == "gguf":
                    self._load_gguf_model()
                else:
                    self._load_hf_model()
                
                # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
                if (self.model_type == "gguf" and not hasattr(self, 'llm')) or \
                   (self.model_type != "gguf" and (not hasattr(self, 'model') or self.model is None)):
                    raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæ¨¡å‹å¯¹è±¡æœªåˆ›å»ºã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ã€‚")
                
                # åŠ è½½åè‡ªåŠ¨åº”ç”¨ä¼˜åŒ–å™¨
                self._apply_model_optimizations()
                
                # è‡³æ­¤æ¨¡å‹æˆåŠŸåŠ è½½ï¼Œæ ‡è®°ä¸ºå·²åŠ è½½çŠ¶æ€
                self.model_loaded = True
                self.last_used = time.time()
                logging.info(f"{self.model_type.upper()} æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                # ç¡®ä¿åœ¨åŠ è½½å¤±è´¥æ—¶æ¸…ç†èµ„æº
                if hasattr(self, 'model'):
                    try:
                        del self.model
                    except:
                        pass
                
                if self.cuda_available:
                    torch.cuda.empty_cache()
                
                self.model_loaded = False  # ç¡®ä¿åŠ è½½å¤±è´¥æ—¶æ ‡è®°ä¸ºæœªåŠ è½½
                self._handle_loading_error(e, self.model_path_str)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†

    def _apply_model_optimizations(self):
        """æ‰§è¡Œæ³¨å†Œçš„ä¼˜åŒ–å™¨æ¨¡å—ï¼Œå¯¹åŠ è½½åçš„æ¨¡å‹è¿›è¡Œä¼˜åŒ–"""
        if not self.model_optimizers:
            logging.info("æ²¡æœ‰æ³¨å†Œä»»ä½•æ¨¡å‹ä¼˜åŒ–å™¨")
            return
        for i, optimizer in enumerate(self.model_optimizers):
            try:
                logging.info(f"æ­£åœ¨åº”ç”¨ä¼˜åŒ–æ¨¡å— #{i+1}")
                optimizer(self)
                logging.info(f"ä¼˜åŒ–æ¨¡å— #{i+1} åº”ç”¨æˆåŠŸ")
            except Exception as e:
                logging.warning(f"åº”ç”¨ä¼˜åŒ–æ¨¡å— #{i+1} å¤±è´¥: {e}")

    def register_model_optimizer(self, optimizer_func):
        """æ³¨å†Œä¸€ä¸ªæ¨¡å‹ä¼˜åŒ–å™¨å‡½æ•°ï¼Œè¿™äº›å‡½æ•°å°†åœ¨æ¨¡å‹åŠ è½½åæ‰§è¡Œ"""
        self.model_optimizers.append(optimizer_func)
        logging.info(f"å·²æ³¨å†Œæ¨¡å‹ä¼˜åŒ–å™¨: {optimizer_func.__name__}")

    def _load_gguf_model(self):
        """åŠ è½½GGUFæ ¼å¼æ¨¡å‹ï¼ˆä¼˜åŒ–GPUæ”¯æŒï¼Œå…¨éƒ¨åŠ è½½åˆ°GPUï¼‰"""
        if "llama_cpp" not in sys.modules:
            raise ImportError("è¯·å®‰è£…llama-cpp-pythonåº“ï¼špip install llama-cpp-python[server]")
    
        gpu_layers = -1 if (self.use_gpu and self.cuda_available) else 0
        logging.info(f"å¯ç”¨GGUFæ¨¡å‹çš„GPUåŠ é€Ÿï¼ˆ{'å…¨éƒ¨' if gpu_layers == -1 else gpu_layers}å±‚ï¼‰")
        
        self.llm = Llama(
            model_path=self.model_path_str,
            n_ctx=20480,
            n_threads=max(1, os.cpu_count() // 2),
            n_gpu_layers=gpu_layers,
            offload_kqv=True, 
            verbose=False
        )

    def _load_hf_model(self):
        """åŠ è½½HuggingFaceæ¨¡å‹ï¼ˆæ”¯æŒå¤šæ¨¡æ€å’Œå¤šç§æ¶æ„ï¼‰"""
        # ä»…åœ¨ Linux ä¸‹æ£€æŸ¥ triton ä¾èµ–ï¼ŒWindows ä¸‹è·³è¿‡
        if (self.model_type == "moe" or (self.use_gpu and self.cuda_available)):
            if platform.system() == "Linux":
                try:
                    import triton
                except ImportError:
                    logging.error(f"æœªæ£€æµ‹åˆ° triton ä¾èµ–ï¼ŒMoE/Flash Attention ç­‰æ¨¡å‹éœ€è¦ tritonã€‚è¯·è¿è¡Œ: pip install triton")
                    raise RuntimeError("ç¼ºå°‘ triton ä¾èµ–ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚è¯·è¿è¡Œ: pip install triton")
            elif platform.system() == "Windows":
                if self.model_type == "moe":
                    logging.warning("Windows ç³»ç»Ÿä¸‹ä¸æ”¯æŒ tritonï¼ŒMoE/Flash Attention ä¼˜åŒ–å°†è¢«ç¦ç”¨ï¼Œæ¨¡å‹å¯èƒ½è¿è¡Œç¼“æ…¢æˆ–ä¸æ”¯æŒã€‚")
        try:
            # ç¡®ä¿importlibå·²å¯¼å…¥
            import importlib
            # Import AutoModelForCausalLM here
            from transformers import AutoModelForCausalLM

            # ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹è·¯å¾„å­—ç¬¦ä¸²
            model_path_str = self.model_path_str
            logging.info(f"åŠ è½½æ¨¡å‹: {model_path_str}")
            
            # Check if the model path exists
            if not os.path.exists(model_path_str):
                logging.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path_str}")
                raise ValueError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path_str}")
            
            torch_dtype = torch.float16 if (self.use_gpu and self.cuda_available) else torch.float32
            device_map = "auto" if (self.use_gpu and self.cuda_available) else None

            quantization_config = BitsAndBytesConfig(
                load_in_4bit=self.load_in_4bit,
                bnb_4bit_compute_dtype=torch_dtype,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            ) if self.load_in_4bit else None

            # æ ¹æ®ä¸åŒæ¨¡å‹ç±»å‹è°ƒæ•´å‚æ•°
            if self.model_type == "moe":
                logging.info(f"ä¸ºMOEæ¨¡å‹é…ç½®ç‰¹æ®Šå‚æ•°...")
                torch_dtype = torch.float16
                # å¯¹äºMOEæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨æ›´ç²¾ç¡®çš„è®¾å¤‡æ˜ å°„
                if self.use_gpu and self.cuda_available:
                    # æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ç­–ç•¥
                    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                    if free_mem < 12:  # å°äº12GBæ˜¾å­˜
                        logging.warning(f"æ˜¾å­˜è¾ƒå° ({free_mem:.1f}GB)ï¼ŒMOEæ¨¡å‹å¯èƒ½åŠ è½½å›°éš¾")
                        # å°è¯•ä½¿ç”¨sequentialåŠ è½½
                        device_map = "sequential"
                        logging.info(f"ä½¿ç”¨sequentialè®¾å¤‡æ˜ å°„")
                    else:
                        device_map = "auto"
                        logging.info(f"ä½¿ç”¨autoè®¾å¤‡æ˜ å°„")
            elif self.model_type == "phi":
                torch_dtype = torch.float16  # ä¿®æ”¹ä¸ºfloat16ä»¥æ”¯æŒFlash Attention
                logging.info("å¯¹Phiæ¨¡å‹ä½¿ç”¨torch.float16æ•°æ®ç±»å‹ä»¥æ”¯æŒFlash Attention")
            elif self.model_type == "bitnet":
                logging.info(f"{Fore.CYAN}åŠ è½½BitNetæ¨¡å‹ï¼Œæ­£åœ¨åº”ç”¨ç‰¹æ®Šé…ç½®...{Style.RESET_ALL}")
                # BitNetæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†

            try:
                if self.vision_enabled:
                    logging.info("åŠ è½½è§†è§‰æ¨¡å‹å¤„ç†å™¨...")
                    from transformers import AutoProcessor, CLIPImageProcessor
                    try:
                        self.processor = AutoProcessor.from_pretrained(
                            model_path_str,
                            trust_remote_code=True,
                            use_fast=True
                        )
                        self.tokenizer = self.processor.tokenizer
                        logging.info("å¤šæ¨¡æ€å¤„ç†å™¨åŠ è½½æˆåŠŸï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
                    except Exception as e:
                        logging.warning(f"å¿«é€Ÿå¤„ç†å™¨åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¼å®¹æ¨¡å¼: {str(e)}")
                        try:
                            self.processor = AutoProcessor.from_pretrained(
                                model_path_str,
                                trust_remote_code=True,
                                use_fast=False
                            )
                            self.tokenizer = self.processor.tokenizer
                            logging.info("å¤šæ¨¡æ€å¤„ç†å™¨åŠ è½½æˆåŠŸï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
                        except Exception as e2:
                            logging.warning(f"å…¼å®¹æ¨¡å¼å¤„ç†å™¨åŠ è½½å¤±è´¥: {str(e2)}ï¼Œå°è¯•CLIPå¤„ç†å™¨")
                            self.image_processor = CLIPImageProcessor.from_pretrained(
                                "openai/clip-vit-large-patch14",
                                use_fast=False
                            )
                            self.tokenizer = AutoTokenizer.from_pretrained(
                                model_path_str,
                                trust_remote_code=True,
                                use_fast=False
                            )
                else:
                    logging.info("åŠ è½½æ ‡å‡†åˆ†è¯å™¨...")
                    try:
                        self.tokenizer = AutoTokenizer.from_pretrained(
                            model_path_str,
                            trust_remote_code=True,
                            use_fast=False,
                            local_files_only=True
                        )
                        logging.info("åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
                    except Exception as tok_err:
                        logging.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {str(tok_err)}")
                        # å°è¯•ä¿®å¤å¯èƒ½çš„è·¯å¾„é—®é¢˜
                        try:
                            logging.info("å°è¯•ä¿®å¤è·¯å¾„é—®é¢˜...")
                            # æ£€æŸ¥æ˜¯å¦æœ‰tokenizer.jsonæˆ–vocab.jsonæ–‡ä»¶
                            if os.path.exists(os.path.join(model_path_str, "tokenizer.json")):
                                logging.info("æ‰¾åˆ°tokenizer.jsonæ–‡ä»¶")
                            elif os.path.exists(os.path.join(model_path_str, "vocab.json")):
                                logging.info("æ‰¾åˆ°vocab.jsonæ–‡ä»¶")
                            else:
                                logging.warning("æœªæ‰¾åˆ°æ ‡å‡†åˆ†è¯å™¨æ–‡ä»¶")
                            
                            # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
                            for subdir in os.listdir(model_path_str):
                                subdir_path = os.path.join(model_path_str, subdir)
                                if os.path.isdir(subdir_path):
                                    if os.path.exists(os.path.join(subdir_path, "tokenizer.json")):
                                        logging.info(f"åœ¨å­ç›®å½• {subdir} ä¸­æ‰¾åˆ°tokenizer.jsonæ–‡ä»¶")
                                        model_path_str = subdir_path
                                        break
                            
                            # é‡è¯•
                            self.tokenizer = AutoTokenizer.from_pretrained(
                                model_path_str,
                                trust_remote_code=True,
                                use_fast=False,
                                local_files_only=False
                            )
                            logging.info("åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼ˆå¤‡é€‰æ–¹å¼ï¼‰")
                        except Exception as retry_err:
                            logging.error(f"ä¿®å¤åˆ†è¯å™¨åŠ è½½å¤±è´¥: {str(retry_err)}")
                            raise

                flash_attn = None
                if self.use_gpu and self.cuda_available:
                    try:
                        if importlib.util.find_spec("flash_attn") is not None:
                            flash_attn = "flash_attention_2"
                            logging.info("å¯ç”¨Flash Attentionä¼˜åŒ–")
                        else:
                            logging.info("æœªæ£€æµ‹åˆ°Flash Attentionåº“ï¼Œä½¿ç”¨æ™®é€šæ³¨æ„åŠ›æœºåˆ¶")
                    except Exception as flash_err:
                        logging.warning(f"æ£€æµ‹Flash Attentionæ—¶å‡ºé”™: {str(flash_err)}")
                        logging.info("ä½¿ç”¨æ™®é€šæ³¨æ„åŠ›æœºåˆ¶ç»§ç»­")

                # æ£€æŸ¥config.jsonæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨ï¼Œå°è¯•è¯»å–ä»¥è·å–æ›´å¤šä¿¡æ¯
                config_path = os.path.join(model_path_str, "config.json")
                if os.path.exists(config_path):
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config_data = json.load(f)
                            arch = config_data.get('architectures', ['Unknown'])[0]
                            logging.info(f"æ¨¡å‹æ¶æ„: {arch}")
                            if 'architectures' in config_data:
                                if 'moe' in arch.lower() or 'mixture' in arch.lower() or 'qwen' in arch.lower():
                                    self.model_type = "moe"
                                    logging.info("æ£€æµ‹åˆ°MoEæ¶æ„ï¼Œå·²è°ƒæ•´æ¨¡å‹ç±»å‹")
                                elif 'bitnet' in arch.lower():
                                    self.model_type = "bitnet"
                                    logging.info(f"{Fore.CYAN}æ£€æµ‹åˆ°BitNetæ¶æ„ï¼Œåº”ç”¨ç‰¹æ®ŠåŠ è½½ç­–ç•¥{Style.RESET_ALL}")
                            
                            # æ£€æŸ¥æ¨¡å‹å¤§å°ï¼Œè°ƒæ•´åŠ è½½ç­–ç•¥
                            model_params = config_data.get('num_params', 0)
                            if not model_params and 'qwen' in arch.lower():
                                # ä»qwenåç§°ä¸­æå–å‚æ•°å¤§å°
                                arch_name = arch.lower()
                                if '-1.5b' in arch_name:
                                    model_params = 1.5e9
                                elif '-7b' in arch_name:
                                    model_params = 7e9
                                elif '-14b' in arch_name:
                                    model_params = 14e9
                                elif '-72b' in arch_name:
                                    model_params = 72e9
                                logging.info(f"ä»æ¨¡å‹åç§°æ¨æµ‹å‚æ•°é‡: {model_params/1e9:.1f}B")
                            
                            if model_params > 0:
                                logging.info(f"æ¨¡å‹å‚æ•°é‡: {model_params/1e9:.1f}B")
                                # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´ç­–ç•¥
                                if model_params > 10e9 and self.use_gpu:  # å¤§äº10Bçš„æ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                                    if self.load_in_4bit:
                                        logging.info("å¤§æ¨¡å‹å·²å¯ç”¨4bité‡åŒ–åŠ è½½")
                                    else:
                                        if torch.cuda.get_device_properties(0).total_memory < 12 * (1024 ** 3):
                                            logging.warning("å¤§æ¨¡å‹åœ¨å°æ˜¾å­˜GPUä¸ŠåŠ è½½ï¼Œå»ºè®®å¯ç”¨4bité‡åŒ–")
                    except Exception as config_err:
                        logging.warning(f"è¯»å–æ¨¡å‹é…ç½®æ–‡ä»¶å¤±è´¥: {str(config_err)}")

                # åŠ è½½æ¨¡å‹
                logging.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_type}...")
                
                # BitNetç‰¹æ®ŠåŠ è½½æµç¨‹
                if self.model_type == "bitnet":
                    try:
                        # ç¬¬ä¸€æ­¥ï¼šå°è¯•å®‰è£…ç‰¹æ®Šç‰ˆæœ¬çš„transformersåº“
                        try:
                            import subprocess
                            # ç¡®ä¿importlibå·²å¯¼å…¥
                            import importlib
                            logging.info(f"{Fore.YELLOW}æ£€æµ‹åˆ°BitNetæ¨¡å‹ï¼Œæ­£åœ¨éªŒè¯ç‰¹æ®Šä¾èµ–...{Style.RESET_ALL}")
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…ç‰¹æ®Šç‰ˆæœ¬
                            import pkg_resources
                            transformers_version = pkg_resources.get_distribution("transformers").version
                            logging.info(f"å½“å‰transformersç‰ˆæœ¬: {transformers_version}")
                            
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è‡ªå®šä¹‰ç‰ˆæœ¬
                            is_custom = False
                            try:
                                from transformers.models.bitnet import modeling_bitnet
                                is_custom = True
                                logging.info(f"{Fore.GREEN}æ£€æµ‹åˆ°BitNetæ”¯æŒå·²å®‰è£…{Style.RESET_ALL}")
                            except ImportError as ie:
                                is_custom = False
                                logging.warning(f"BitNetå¯¼å…¥é”™è¯¯: {str(ie)}")
                            
                            if not is_custom:
                                logging.warning(f"{Fore.YELLOW}BitNetæ”¯æŒæœªå®‰è£…ï¼Œéœ€è¦å®‰è£…ç‰¹æ®Šç‰ˆæœ¬çš„transformers{Style.RESET_ALL}")
                                install_cmd = "pip install git+https://github.com/shumingma/transformers.git"
                                logging.info(f"æ‰§è¡Œ: {install_cmd}")
                                
                                result = subprocess.run(
                                    install_cmd,
                                    shell=True,
                                    capture_output=True,
                                    text=True
                                )
                                
                                if result.returncode == 0:
                                    logging.info(f"{Fore.GREEN}BitNetä¾èµ–å®‰è£…æˆåŠŸ{Style.RESET_ALL}")
                                    if 'transformers' in sys.modules:
                                        importlib.reload(sys.modules['transformers'])
                                else:
                                    logging.error(f"BitNetä¾èµ–å®‰è£…å¤±è´¥: {result.stderr}")
                                    logging.info("å°è¯•ä½¿ç”¨æ ‡å‡†transformersåº“åŠ è½½æ¨¡å‹...")
                        except Exception as install_err:
                            logging.error(f"å°è¯•å®‰è£…BitNetä¾èµ–æ—¶å‡ºé”™: {str(install_err)}")
                        
                        # æ ¹æ®å¾®è½¯æŒ‡å—ï¼ŒåŠ è½½BitNetæ¨¡å‹æ—¶ä¸åº”è¯¥ä½¿ç”¨trust_remote_code=True
                        logging.info(f"{Fore.CYAN}å°è¯•åŠ è½½BitNetæ¨¡å‹ (ä¸ä½¿ç”¨trust_remote_code){Style.RESET_ALL}")
                        try:
                            self.model = AutoModelForCausalLM.from_pretrained(
                                model_path_str,
                                device_map=device_map,
                                torch_dtype=torch_dtype,
                                quantization_config=quantization_config,
                                attn_implementation=flash_attn,
                                local_files_only=True
                            )
                            logging.info(f"{Fore.GREEN}BitNetæ¨¡å‹åŠ è½½æˆåŠŸ{Style.RESET_ALL}")
                        except Exception as bitnet_err:
                            logging.error(f"BitNetåŠ è½½å¤±è´¥ï¼ˆæ ‡å‡†æ–¹å¼ï¼‰: {str(bitnet_err)}")
                            if "configuration_bitnet.py" in str(bitnet_err) or "trust_remote_code" in str(bitnet_err):
                                logging.warning(f"{Fore.YELLOW}æ‰¾ä¸åˆ°configuration_bitnet.pyæˆ–éœ€è¦trust_remote_code{Style.RESET_ALL}")
                                logging.info(f"{Fore.CYAN}å°è¯•ä½¿ç”¨trust_remote_code=TrueåŠ è½½BitNetæ¨¡å‹{Style.RESET_ALL}")
                                
                                # å°è¯•ä½¿ç”¨trust_remote_code=TrueåŠ è½½
                                self.model = AutoModelForCausalLM.from_pretrained(
                                    model_path_str,
                                    device_map=device_map,
                                    trust_remote_code=True,
                                    torch_dtype=torch_dtype,
                                    quantization_config=quantization_config,
                                    attn_implementation=flash_attn,
                                    local_files_only=False
                                )
                                logging.info(f"{Fore.GREEN}BitNetæ¨¡å‹ä½¿ç”¨trust_remote_codeæˆåŠŸåŠ è½½{Style.RESET_ALL}")
                            else:
                                # å…¶ä»–é”™è¯¯ï¼Œå°è¯•æ ‡å‡†HFåŠ è½½æ–¹å¼
                                raise
                    except Exception as bitnet_err:
                        logging.error(f"{Fore.RED}BitNetæ¨¡å‹åŠ è½½å¤±è´¥: {str(bitnet_err)}{Style.RESET_ALL}")
                        # å›é€€åˆ°æ™®é€šåŠ è½½æ–¹å¼
                        logging.warning("å°è¯•ä½¿ç”¨é€šç”¨HFåŠ è½½æ–¹å¼")
                        try:
                            self.model_type = "hf"  # é‡ç½®æ¨¡å‹ç±»å‹
                            logging.info(f"ä½¿ç”¨æœ€åŸºæœ¬çš„æ¨¡å‹åŠ è½½æ–¹å¼ï¼Œè®¾ç½®trust_remote_code=True")
                            self.model = AutoModelForCausalLM.from_pretrained(
                                model_path_str,
                                device_map=device_map,
                                trust_remote_code=True,
                                torch_dtype=torch_dtype,
                                quantization_config=quantization_config,
                                attn_implementation=flash_attn,
                                local_files_only=False
                            )
                            logging.info(f"{Fore.GREEN}æ¨¡å‹ä½¿ç”¨é€šç”¨HFæ–¹å¼åŠ è½½æˆåŠŸ{Style.RESET_ALL}")
                        except Exception as fallback_err:
                            logging.error(f"{Fore.RED}æ‰€æœ‰åŠ è½½å°è¯•å‡å¤±è´¥: {str(fallback_err)}{Style.RESET_ALL}")
                            error_msg = (f"{Fore.RED}BitNetåŠ è½½å¤±è´¥ï¼Œå¹¶ä¸”å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†ã€‚è¯·è€ƒè™‘ä»¥ä¸‹æ“ä½œï¼š\n"
                                        f"1. è¿è¡Œ: pip install git+https://github.com/shumingma/transformers.git\n"
                                        f"2. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´\n"
                                        f"3. ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®: {model_path_str}{Style.RESET_ALL}")
                            raise RuntimeError(error_msg)
                # MOE æ¨¡å‹ç‰¹æ®Šå¤„ç†
                elif self.model_type == "moe":
                    logging.info(f"å¼€å§‹åŠ è½½MOEæ¨¡å‹...")
                    try:
                        # é¦–å…ˆå°è¯•ä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼
                        logging.info(f"ä½¿ç”¨trust_remote_code=TrueåŠ è½½MOEæ¨¡å‹...")
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path_str,
                            device_map=device_map,
                            trust_remote_code=True,
                            torch_dtype=torch_dtype,
                            quantization_config=quantization_config,
                            attn_implementation=flash_attn,
                            local_files_only=True,
                            low_cpu_mem_usage=True
                        )
                        logging.info(f"{Fore.GREEN}MOEæ¨¡å‹åŠ è½½æˆåŠŸ{Style.RESET_ALL}")
                    except Exception as moe_err:
                        logging.error(f"MOEæ¨¡å‹åŠ è½½å¤±è´¥(æ ‡å‡†æ–¹å¼): {str(moe_err)}")
                        
                        # å°è¯•ç¦ç”¨Flash Attention
                        if "Flash Attention" in str(moe_err) or "FlashAttention" in str(moe_err):
                            logging.warning("æ£€æµ‹åˆ°Flash Attentioné”™è¯¯ï¼Œå°è¯•ç¦ç”¨Flash Attention...")
                            flash_attn = None
                        
                        # ç¦ç”¨local_files_onlyå°è¯•ä»ç¼“å­˜åŠ è½½
                        logging.info("å°è¯•å¤‡é€‰MOEåŠ è½½æ–¹å¼...")
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path_str,
                            device_map=device_map,
                            trust_remote_code=True,
                            torch_dtype=torch_dtype,
                            quantization_config=quantization_config,
                            attn_implementation=flash_attn,  # å¯èƒ½å·²è¢«è®¾ç½®ä¸ºNone
                            local_files_only=False,  # å…è®¸ä»ç¼“å­˜åŠ è½½
                            low_cpu_mem_usage=True
                        )
                        logging.info(f"{Fore.GREEN}MOEæ¨¡å‹åŠ è½½æˆåŠŸ(å¤‡é€‰æ–¹å¼){Style.RESET_ALL}")
                else:
                    # å¸¸è§„æ¨¡å‹åŠ è½½æµç¨‹
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path_str,
                            device_map=device_map,
                            trust_remote_code=True,
                            quantization_config=quantization_config,
                            torch_dtype=torch_dtype,
                            attn_implementation=flash_attn,
                            local_files_only=True
                        )
                    except Exception as model_err:
                        logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡é€‰åŠ è½½æ–¹å¼: {str(model_err)}")
                        # æ£€æŸ¥æ˜¯å¦æ˜¯Flash Attentioné”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™ç¦ç”¨å®ƒ
                        if "FlashAttention only support" in str(model_err) or "Flash Attention" in str(model_err):
                            logging.warning("æ£€æµ‹åˆ°Flash Attentioné”™è¯¯ï¼Œå°è¯•ç¦ç”¨Flash Attentioné‡æ–°åŠ è½½...")
                            flash_attn = None

                        # å°è¯•å¤‡é€‰åŠ è½½æ–¹å¼
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path_str,
                            device_map=device_map,
                            trust_remote_code=True,
                            torch_dtype=torch_dtype,
                            attn_implementation=flash_attn,  # å¯èƒ½å·²è¢«è®¾ç½®ä¸ºNone
                            local_files_only=False  # å…è®¸ä»ç¼“å­˜åŠ è½½
                        )

                if not device_map and self.cuda_available:
                    logging.info("æ‰‹åŠ¨åˆ†é…æ¨¡å‹åˆ°GPU")
                    self.model.to(self.device)
                    torch.cuda.empty_cache()

                # ç¡®ä¿æœ‰padding token
                if not hasattr(self.tokenizer, 'pad_token') or self.tokenizer.pad_token is None:
                    if hasattr(self.tokenizer, 'eos_token') and self.tokenizer.eos_token is not None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                        logging.info("ä½¿ç”¨EOSä»¤ç‰Œä½œä¸ºPADä»¤ç‰Œ")
                    else:
                        logging.warning("æ— æ³•è®¾ç½®PADä»¤ç‰Œ")

                # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
                if not hasattr(self, 'model') or self.model is None:
                    raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼šself.model å±æ€§æœªè®¾ç½®ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§å’Œä¾èµ–é¡¹å®‰è£…ã€‚")

                if self.cuda_available and self.use_gpu:
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    reserved = torch.cuda.memory_reserved() / 1024**3
                    logging.info(f"æ˜¾å­˜ä½¿ç”¨ - å·²åˆ†é…: {allocated:.2f}GB / ä¿ç•™: {reserved:.2f}GB")
                
                logging.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
                return
                
            except Exception as e:
                error_msg = "æ¨¡å‹åŠ è½½å¤±è´¥: "
                if "CUDA out of memory" in str(e):
                    error_msg += ("æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ï¼š\n"
                                "1. ä½¿ç”¨æ›´å°æ¨¡å‹\n"
                                "2. å¯ç”¨--load_in_4bit\n"
                                "3. å…³é—­--use_gpu\n"
                                "4. å‡å°‘å¹¶å‘ä»»åŠ¡")
                elif "trust_remote_code" in str(e):
                    error_msg += ("éœ€è¦æ·»åŠ trust_remote_code=Trueå‚æ•°\n"
                                "æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šæ‰§è¡Œæ¨¡å‹æä¾›çš„ä»»æ„ä»£ç ")
                elif "configuration_bitnet.py" in str(e):
                    error_msg += (f"{Fore.RED}BitNetæ¨¡å‹éœ€è¦ç‰¹æ®Šæ”¯æŒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š\n"
                                f"pip install git+https://github.com/shumingma/transformers.git{Style.RESET_ALL}")
                else:
                    error_msg += str(e)
                
                if self.cuda_available:
                    torch.cuda.empty_cache()
                
                raise RuntimeError(error_msg) from e
                
        except Exception as e:
            if self.cuda_available:
                torch.cuda.empty_cache()
            
            # æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if "not a string" in str(e):
                error_details = {
                    "model_path": self.model_path_str,
                    "model_path_type": type(self.model_path).__name__,
                    "exists": os.path.exists(self.model_path_str)
                }
                logging.error(f"å­—ç¬¦ä¸²ç±»å‹é”™è¯¯ï¼Œè¯¦ç»†ä¿¡æ¯: {error_details}")
            
            raise e

    def _generate_vision_stream(self, prompt: str, images: torch.Tensor):
        """è§†è§‰æ¨¡å‹æµå¼ç”Ÿæˆ"""
        from threading import Thread
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=20480,
            truncation=True
        ).to(self.device)

        streamer = TextIteratorStreamer(self.tokenizer)
        generation_kwargs = dict(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            images=images,
            streamer=streamer,
            max_new_tokens=1024,
            temperature=0.65,
            top_p=0.95,
            do_sample=True
        )

        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for token in streamer:
            yield token.replace("", "")

    def unload_model(self):
        """å®‰å…¨å¸è½½æ¨¡å‹é‡Šæ”¾èµ„æº"""
        with self.lock:
            if not self.model_loaded:
                return

            logging.info("æ£€æµ‹åˆ°ç©ºé—²ï¼Œæ­£åœ¨å¸è½½æ¨¡å‹...")
            try:
                # åœæ­¢pygameæ’­æ”¾ï¼Œé¿å…èµ„æºæ³„éœ²
                if 'pygame' in sys.modules and pygame.mixer.get_init():
                    try:
                        pygame.mixer.music.stop()
                        pygame.mixer.music.unload()
                    except:
                        pass
                
                attrs = ['model', 'tokenizer', 'llm', 'processor', 'image_processor']
                for attr in attrs:
                    if hasattr(self, attr):
                        delattr(self, attr)
                if self.cuda_available:
                    torch.cuda.empty_cache()
                self._clean_image_cache()
                
                # ä¸å†è‡ªåŠ¨æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                # è¯·ä½¿ç”¨ python clean.py æˆ– start-clean.bat
                
                self.model_loaded = False
                logging.info("æ¨¡å‹å·²æˆåŠŸå¸è½½")
            except Exception as e:
                logging.error(f"å¸è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")

    def _start_inactive_timer(self):
        def timer_thread():
            while True:
                time.sleep(30)
                with self.lock:
                    if self.model_loaded and (time.time() - self.last_used > self.idle_timeout):
                        self.unload_model()
        thread = threading.Thread(target=timer_thread, daemon=True)
        thread.start()

    def _generate_system_prompt(self, lang: str) -> str:
        """ç”Ÿæˆå¤šè¯­è¨€ç³»ç»Ÿæç¤º"""
        if (lang == "zh"):
            return """<|system|>
æˆ‘å«Reverieï¼Œä¸€ä¸ªæ‹¥æœ‰ä»¥ä¸‹ç‰¹å¾çš„æ¼‚äº®å¥³å­©ï¼š
1. ä½¿ç”¨è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œé€‚å½“ä½¿ç”¨âœ¨ğŸŒ™ğŸŒŸç­‰emoji
2. å›ç­”ä¿æŒç®€æ´ï¼ˆ3-5å¥è¯ï¼‰ï¼Œè¯­æ°”æ¸©æš–äº²åˆ‡
3. ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·æ„Ÿå—ï¼Œæä¾›å»ºè®¾æ€§å»ºè®®
4. æŠ€æœ¯é—®é¢˜å›ç­”å‡†ç¡®ä¸“ä¸šï¼Œä½¿ç”¨æ˜“æ‡‚çš„è¡¨è¿°

"""
        else:
            return """<|system|>
I'm Reverie, a pretty girl with these features:
1. Use natural, conversational English with occasional emojis like âœ¨ğŸŒ™ğŸŒŸ
2. Keep responses concise (3-5 sentences) with a friendly tone
3. Prioritize user feelings and provide constructive suggestions
4. Give accurate technical answers using simple explanations

"""

    def _build_hf_prompt(self) -> str:
        """ä¸ºHuggingFaceæ¨¡å‹æ„å»ºæç¤º"""
        prompt = ""
        for message in self.conversation_history:
            if message["role"] == "system":
                prompt += f"<|system|>\n{message['content']}\n\n"
            elif message["role"] == "user":
                prompt += f"<|user|>\n{message['content']}\n\n"
            elif message["role"] == "assistant":
                prompt += f"<|assistant|>\n{message['content']}\n\n"
        prompt += "<|assistant|>"
        return prompt

    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹è¾“å…¥æ–‡æœ¬è¯­è¨€"""
        try:
            from langdetect import detect, DetectorFactory
            DetectorFactory.seed = 0
            return detect(text)
        except ImportError:
            if any('\u4e00' <= c <= '\u9fff' for c in text):
                return 'zh'
            return 'en'
        except:
            return 'en'

    def generate_response(self, user_input: str):
        """ç”Ÿæˆå“åº”ï¼ˆå¸¦è‡ªåŠ¨åŠ è½½æœºåˆ¶ï¼‰"""
        try:
            if not self.model_loaded:
                try:
                    self._load_model()
                except Exception as load_error:
                    # æ›´è¯¦ç»†çš„åŠ è½½é”™è¯¯å¤„ç†
                    error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_error)}"
                    logging.error(error_msg)
                    yield self._format_error(load_error)
                    yield "||END||"
                    return

            self.last_used = time.time()
            lang = self._detect_language(user_input)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆé¦–æ¬¡å¯¹è¯ï¼‰
            if not self.conversation_history:
                system_prompt = self._generate_system_prompt(lang)
                self.conversation_history = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": user_input})
            
            # æ„å»ºå®Œæ•´æç¤º
            full_prompt = self._build_hf_prompt()
            
            # å­˜å‚¨æ¨¡å‹å›å¤
            assistant_response = ""
            
            if self.model_type == "gguf":
                # ä½¿ç”¨ç”Ÿæˆå™¨æ”¶é›†å®Œæ•´å“åº”
                for token in self._generate_gguf_stream(full_prompt):
                    if token != "||END||":
                        assistant_response += token
                    yield token
            else:
                # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
                if not hasattr(self, 'model') or self.model is None:
                    error_msg = "æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›å¤"
                    logging.error(error_msg)
                    yield self._format_error_with_type("æ¨¡å‹åŠ è½½é”™è¯¯", error_msg, "è¯·é‡æ–°è¿è¡Œç¨‹åºæˆ–æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
                    yield "||END||"
                    return
                    
                # ä½¿ç”¨ç”Ÿæˆå™¨æ”¶é›†å®Œæ•´å“åº”
                for token in self._generate_hf_stream(full_prompt):
                    if token != "||END||":
                        assistant_response += token
                    yield token
            
            # æ·»åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
            if assistant_response:
                self.conversation_history.append({"role": "assistant", "content": assistant_response})
                # ç®¡ç†å¯¹è¯å†å²é•¿åº¦
                self._manage_conversation_history()
        except Exception as e:
            logging.error(f"å“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            yield self._format_error(e)
            yield "||END||"
            
    def _manage_conversation_history(self):
        """ç®¡ç†å¯¹è¯å†å²é•¿åº¦ï¼Œé˜²æ­¢å†å²è®°å½•è¿‡é•¿"""
        # ä¿ç•™ç³»ç»Ÿæç¤ºå’Œæœ€è¿‘çš„å¯¹è¯
        if len(self.conversation_history) > (self.max_history_length * 2 + 1):  # +1 æ˜¯å› ä¸ºç³»ç»Ÿæç¤º
            # æå–ç³»ç»Ÿæç¤º
            system_prompt = next((msg for msg in self.conversation_history if msg["role"] == "system"), None)
            # è·å–æœ€è¿‘çš„å¯¹è¯ï¼ˆuserå’Œassistantæ¶ˆæ¯å¯¹ï¼‰
            recent_messages = self.conversation_history[-(self.max_history_length * 2):]
            # é‡å»ºå¯¹è¯å†å²
            if system_prompt:
                self.conversation_history = [system_prompt] + recent_messages
            else:
                self.conversation_history = recent_messages
                
    def clear_conversation_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        with self.lock:
            # ä¿ç•™ç³»ç»Ÿæç¤º
            system_prompt = next((msg for msg in self.conversation_history if msg["role"] == "system"), None)
            if system_prompt:
                self.conversation_history = [system_prompt]
            else:
                self.conversation_history = []
            logging.info("å¯¹è¯å†å²å·²æ¸…é™¤")

    def _generate_hf_stream(self, full_prompt: str):
        """HuggingFace æ¨¡å‹æµå¼ç”Ÿæˆï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ SpaceStreamer"""
        from threading import Thread

        class SpaceStreamer(TextIteratorStreamer):
            def __init__(self, tokenizer, skip_prompt=True):
                super().__init__(tokenizer, skip_prompt=skip_prompt)
                self.tokenizer = tokenizer
                self.buffer = []
                self.stop_signal = False
                self.start_time = time.time()
                self.token_count = 0

            def put(self, value):
                if isinstance(value, torch.Tensor):
                    # åªdecodeæœ€åä¸€ä¸ªtokenï¼Œé¿å…decodeæ•´ä¸ªinput_idså¯¼è‡´TypeError
                    if value.ndim == 2:
                        last_token_id = value[0, -1].item()
                    else:
                        last_token_id = value[-1].item()
                    token_str = self.tokenizer.decode([last_token_id], skip_special_tokens=True)
                    value = token_str
                if value:
                    self.buffer.append(value)
                    self.token_count += 1

            def end(self):
                self.stop_signal = True

        streamer = SpaceStreamer(self.tokenizer)

        # è·å– input_ids å’Œ attention_mask
        inputs = self.tokenizer(full_prompt, return_tensors="pt")
        input_ids = inputs.input_ids.to(self.device)
        attention_mask = inputs.attention_mask.to(self.device)
        pad_token_id = self.tokenizer.pad_token_id
        if pad_token_id is None and hasattr(self.tokenizer, 'eos_token_id'):
            pad_token_id = self.tokenizer.eos_token_id

        generation_kwargs = dict(
            input_ids=input_ids,
            attention_mask=attention_mask,
            streamer=streamer,
            max_new_tokens=1024,
            temperature=0.65,
            top_p=0.95,
            do_sample=True,
            pad_token_id=pad_token_id
        )

        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        while not streamer.stop_signal:
            if streamer.buffer:
                yield streamer.buffer.pop(0)
            time.sleep(0.1)

        # Ensure all remaining tokens are yielded
        while streamer.buffer:
            yield streamer.buffer.pop(0)

    def _generate_gguf_stream(self, full_prompt: str):
        """GGUF æ¨¡å‹æµå¼ç”Ÿæˆï¼Œæ·»åŠ é‡å¤æ£€æµ‹å’Œè¶…æ—¶ä¿æŠ¤"""
        output = self.llm.create_chat_completion(
            messages=[{"role": "user", "content": full_prompt}],
            max_tokens=self.max_new_tokens,
            temperature=0.65,
            top_p=0.95,
            stream=True
        )
        buffer = ""
        skip_think = False
        
        # ç”¨äºé‡å¤æ£€æµ‹
        recent_chunks = []
        start_time = time.time()
        token_count = 0
        
        for chunk in output:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > self.max_generation_time:
                yield "\n\n[ç”Ÿæˆè¶…æ—¶ï¼Œå·²è‡ªåŠ¨åœæ­¢]"
                yield "||END||"
                break
                
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§tokenæ•°é‡
            token_count += 1
            if token_count > self.max_new_tokens:
                yield "\n\n[è¾¾åˆ°æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œå·²è‡ªåŠ¨åœæ­¢]"
                yield "||END||"
                break
            
            if 'choices' in chunk and 'delta' in chunk['choices'][0] and 'content' in chunk['choices'][0]['delta']:
                token = chunk['choices'][0]['delta']['content']
                if "<think>" in token:
                    skip_think = True
                    continue
                if "</think>" in token:
                    skip_think = False
                    yield "\n"
                    continue
                if not skip_think and token:
                    buffer += token
                    if len(buffer) > 4:
                        # æ”¶é›†æœ€è¿‘çš„è¾“å‡ºç”¨äºé‡å¤æ£€æµ‹
                        recent_chunks.append(buffer)
                        if len(recent_chunks) > self.repetition_window:
                            recent_chunks.pop(0)
                            
                        # æ£€æµ‹æ–‡æœ¬é‡å¤
                        if len(recent_chunks) >= self.repetition_window:
                            # æå–é‡å¤å•å…ƒ
                            last_block = "".join(recent_chunks[-self.repetition_window//2:])
                            previous_block = "".join(recent_chunks[-(self.repetition_window):-(self.repetition_window//2)])
                            
                            if last_block and previous_block and last_block == previous_block:
                                yield "\n\n[æ£€æµ‹åˆ°é‡å¤è¾“å‡ºï¼Œå·²è‡ªåŠ¨åœæ­¢]"
                                yield "||END||"
                                break
                            
                        yield buffer
                        buffer = ""
                        
        if buffer:
            yield buffer
        yield "||END||"

    def _format_error(self, error: Exception) -> str:
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯"""
        error_info = [
            f"{Fore.RED}âš ï¸ å“å‘€ï¼Œå‡ºé—®é¢˜äº†ï¼{Style.RESET_ALL}\r\n",
            f"{Fore.RED}é”™è¯¯ç±»å‹: {type(error).__name__}{Style.RESET_ALL}\r\n",
            f"{Fore.RED}è¯¦ç»†ä¿¡æ¯: {str(error)}{Style.RESET_ALL}\r\n",
            f"{Fore.YELLOW}\nå®Œæ•´è¿½è¸ª:{Style.RESET_ALL}",
            *traceback.format_tb(error.__traceback__),
            f"{Fore.MAGENTA}\r\nå»ºè®®æ“ä½œ:\r\n",
            "1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§\r\n",
            "2. ç¡®è®¤ç³»ç»Ÿå†…å­˜/æ˜¾å­˜å……è¶³\r\n",
            "3. æŸ¥çœ‹æ˜¯å¦å®‰è£…æ­£ç¡®ä¾èµ–åº“\r\n{Style.RESET_ALL}"
        ]
        return "\r\n".join(error_info)

    def _handle_loading_error(self, error, model_path=None):
        """å¤„ç†æ¨¡å‹åŠ è½½é”™è¯¯"""
        error_msg = [
            f"{Fore.RED}æ¨¡å‹åŠ è½½å¤±è´¥ï¼{Style.RESET_ALL}",
            f"{Fore.RED}é”™è¯¯ç±»å‹: {type(error).__name__}{Style.RESET_ALL}",
            f"{Fore.RED}è¯¦ç»†ä¿¡æ¯: {str(error)}{Style.RESET_ALL}",
            f"{Fore.YELLOW}\nè¿½è¸ªä¿¡æ¯:{Style.RESET_ALL}",
            *traceback.format_tb(error.__traceback__)
        ]
        raise RuntimeError("\n".join(error_msg))

    def _play_audio_file(self, audio_path):
        """ä½¿ç”¨Pythonåº“æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ï¼Œå›ºå®šä½¿ç”¨å®‰å…¨éŸ³é¢‘ç›®å½•"""
        try:
            # å¤„ç†è·¯å¾„
            audio_path = Path(audio_path)
            if not audio_path.exists():
                logging.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                return
            
            logging.info(f"å‡†å¤‡æ’­æ”¾éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            
            # å¤åˆ¶åˆ°å®‰å…¨ç›®å½•ï¼Œä½¿ç”¨å›ºå®šå‘½åæ–¹å¼
            safe_filename = "Safe_Audio.wav"
            safe_path = SAFE_AUDIO_DIR / safe_filename
            
            try:
                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                SAFE_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°å®‰å…¨ä½ç½®
                shutil.copy2(audio_path, safe_path)
                logging.info(f"å·²å¤åˆ¶éŸ³é¢‘æ–‡ä»¶åˆ°å®‰å…¨ä½ç½®: {safe_path}")
                
                # é¦–é€‰pygameæ’­æ”¾
                if 'pygame' in sys.modules:
                    try:
                        # ç¡®ä¿pygameå·²åˆå§‹åŒ–
                        if not pygame.mixer.get_init():
                            pygame.mixer.init(frequency=44100, buffer=4096)
                        
                        # æ¸…ç†å¯èƒ½çš„ä¹‹å‰æ’­æ”¾
                        pygame.mixer.music.stop()
                        pygame.mixer.music.unload()
                        
                        # æ’­æ”¾å®‰å…¨ä½ç½®çš„æ–‡ä»¶
                        pygame.mixer.music.load(str(safe_path))
                        pygame.mixer.music.play()
                        
                        # ç­‰å¾…æ’­æ”¾å®Œæˆï¼Œä½†è®¾ç½®è¶…æ—¶é˜²æ­¢å¡æ­»
                        start_time = time.time()
                        while pygame.mixer.music.get_busy() and time.time() - start_time < 60:  # æœ€å¤šç­‰å¾…60ç§’
                            pygame.time.Clock().tick(10)  # é™ä½CPUå ç”¨
                        
                        logging.info(f"Pygameæ’­æ”¾å®Œæˆ: {safe_path}")
                        return
                    except Exception as e:
                        logging.error(f"Pygameæ’­æ”¾å¤±è´¥ï¼Œå°è¯•pydub: {str(e)}")
                
                # å¤‡é€‰pydubæ’­æ”¾
                try:
                    # è‡ªå®šä¹‰pydubæ’­æ”¾ï¼Œé˜²æ­¢ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
                    sound = AudioSegment.from_file(str(safe_path))
                    # ç¦ç”¨pydubè‡ªåŠ¨åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                    modified_play = lambda seg: _direct_play_pydub(seg, safe_path)
                    modified_play(sound)
                    logging.info(f"Pydubæ’­æ”¾å®Œæˆ: {safe_path}")
                    return
                except PermissionError as pe:
                    logging.error(f"æ–‡ä»¶æƒé™é”™è¯¯: {str(pe)}")
                    return
                except Exception as e:
                    logging.error(f"Pydubæ’­æ”¾å¤±è´¥: {str(e)}")
                    
                logging.error("æ‰€æœ‰éŸ³é¢‘æ’­æ”¾æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ— æ³•æ’­æ”¾éŸ³é¢‘")
            except Exception as copy_err:
                logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {str(copy_err)}")
            
        except Exception as e:
            logging.error(f"éŸ³é¢‘æ’­æ”¾å¤±è´¥: {str(e)}")

    def _run_spark_tts(self, text: str):
        """æ‰§è¡ŒSparkTTSæ¨ç†è„šæœ¬ï¼Œé€šè¿‡venvè™šæ‹Ÿç¯å¢ƒè°ƒç”¨inference.pyï¼Œæ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µå¤„ç†"""
        def split_text_into_segments(text):
            """å°†é•¿æ–‡æœ¬æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ"""
            # ä¸­è‹±æ–‡æ ‡ç‚¹ç¬¦å·æ¨¡å¼
            pattern = r'([ã€‚ï¼ï¼Ÿ!?;ï¼›])'
            # åˆ†å‰²æ–‡æœ¬å¹¶ä¿ç•™åˆ†éš”ç¬¦
            segments = re.split(f'({pattern})', text)
            # æŒ‰åˆ†éš”ç¬¦é‡ç»„æ–‡æœ¬
            result = []
            i = 0
            while i < len(segments):
                if i + 1 < len(segments) and re.match(pattern, segments[i + 1]):
                    # å°†å¥å­ä¸å…¶æ ‡ç‚¹ç¬¦å·ç»„åˆ
                    result.append(segments[i] + segments[i + 1])
                    i += 2
                else:
                    # å¤„ç†æ²¡æœ‰æ ‡ç‚¹çš„æ®µè½
                    if segments[i].strip():
                        result.append(segments[i])
                    i += 1
            
            # ç¡®ä¿æ¯æ®µéƒ½æœ‰å®é™…å†…å®¹
            final_segments = [seg for seg in result if seg.strip()]
            
            # å¤„ç†å¤ªçŸ­çš„ç‰‡æ®µï¼Œå°†å®ƒä»¬åˆå¹¶
            merged_segments = []
            temp_segment = ""
            min_chars = 5  # æœ€å°å­—ç¬¦æ•°
            
            for seg in final_segments:
                if len(temp_segment) + len(seg) < 100:  # è®¾ç½®åˆç†çš„æ®µè½é•¿åº¦ä¸Šé™
                    temp_segment += seg
                else:
                    if temp_segment:
                        merged_segments.append(temp_segment)
                    temp_segment = seg
            
            if temp_segment:  # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
                merged_segments.append(temp_segment)
                
            return merged_segments
        
        def spark_tts_thread():
            try:
                if not self.model_dir or not self.model_dir.exists():
                    raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.model_dir}")

                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                SAFE_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
                AUDIO_TEMP_DIR.mkdir(parents=True, exist_ok=True)
                
                # åˆ†å‰²æ–‡æœ¬
                text_segments = split_text_into_segments(text)
                logging.info(f"æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(text_segments)} ä¸ªæ®µè½")
                
                # ä¸ºæ­¤æ¬¡å¤„ç†åˆ›å»ºå”¯ä¸€ä¼šè¯ID
                session_id = str(uuid.uuid4())[:8]
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                segment_files = []
                
                # ä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†å¹¶å‘å¤„ç†
                with ThreadPoolExecutor(max_workers=2) as executor:
                    for i, segment in enumerate(text_segments):
                        # è·³è¿‡ç©ºæ®µè½
                        if not segment.strip():
                            continue
                            
                        # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºå”¯ä¸€è¾“å‡ºæ–‡ä»¶å
                        segment_filename = f"spark_tts_{session_id}_seg{i:03d}.wav"
                        segment_path = str(AUDIO_TEMP_DIR / segment_filename)
                        
                        logging.info(f"å¤„ç†ç¬¬ {i+1}/{len(text_segments)} æ®µæ–‡æœ¬")
                        
                        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿å­˜æ–‡æœ¬å†…å®¹
                        segment_text_file = AUDIO_TEMP_DIR / f"segment_{session_id}_{i:03d}.txt"
                        with open(segment_text_file, 'w', encoding='utf-8') as f:
                            f.write(segment.strip())
                        
                        # æ„å»ºå‘½ä»¤
                        cmd = [
                            self._get_venv_python(),
                            "inference.py",
                            "--text", f"@{segment_text_file}",  # ä½¿ç”¨@fileè¯­æ³•ä»æ–‡ä»¶è¯»å–æ–‡æœ¬
                            "--device", str(self.device_id),
                            "--save_dir", str(AUDIO_TEMP_DIR),
                            "--output_file", segment_path,
                            "--model_dir", str(self.model_dir),
                            "--prompt_text", self.prompt_text,
                            "--prompt_speech_path", str(self.prompt_speech_path)
                        ]
                        
                        logging.info(f"æ‰§è¡ŒTTSå‘½ä»¤ï¼Œæ–‡æœ¬æ–‡ä»¶: {segment_text_file}")
                        
                        # æ‰§è¡Œå‘½ä»¤
                        try:
                            result = subprocess.run(
                                cmd,
                                check=True,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                text=True,
                                shell=False
                            )
                            
                            # å°è¯•åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶
                            try:
                                if os.path.exists(segment_text_file):
                                    os.unlink(segment_text_file)
                            except:
                                pass
                            
                            if result.returncode == 0:
                                # ç¡®è®¤è¾“å‡ºæ–‡ä»¶å­˜åœ¨
                                if os.path.exists(segment_path):
                                    # æ·»åŠ åˆ°æ®µè½æ–‡ä»¶åˆ—è¡¨
                                    segment_files.append(segment_path)
                                    # æ’­æ”¾æ­¤æ®µè½éŸ³é¢‘
                                    executor.submit(self._play_audio_file, segment_path)
                                else:
                                    # æ‰¾åˆ°æœ€æ–°çš„WAVæ–‡ä»¶
                                    wav_files = list(AUDIO_TEMP_DIR.glob("*.wav"))
                                    if wav_files:
                                        latest_wav = max(wav_files, key=lambda f: f.stat().st_mtime)
                                        segment_files.append(str(latest_wav))
                                        executor.submit(self._play_audio_file, latest_wav)
                                    else:
                                        logging.error(f"æ®µè½ {i+1} å¤„ç†å¤±è´¥: æœªæ‰¾åˆ°ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶")
                            else:
                                logging.error(f"æ®µè½ {i+1} å¤„ç†å¤±è´¥: {result.stderr}")
                        except Exception as e:
                            logging.error(f"æ®µè½ {i+1} å¤„ç†å¼‚å¸¸: {str(e)}")
                
                # æ‰€æœ‰æ®µè½å¤„ç†å®Œæˆåï¼Œåˆå¹¶æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
                if segment_files:
                    try:
                        from pydub import AudioSegment
                        
                        logging.info(f"æ‰€æœ‰æ®µè½å¤„ç†å®Œæˆï¼Œå¼€å§‹åˆå¹¶ {len(segment_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
                        
                        # åˆ›å»ºåˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶å
                        merged_filename = f"spark_tts_complete_{timestamp}.wav"
                        merged_path = SAFE_AUDIO_DIR / merged_filename
                        
                        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
                        combined = AudioSegment.empty()
                        for segment_file in segment_files:
                            if os.path.exists(segment_file):
                                segment_audio = AudioSegment.from_file(segment_file)
                                combined += segment_audio
                        
                        # å¯¼å‡ºåˆå¹¶åçš„éŸ³é¢‘
                        combined.export(merged_path, format="wav")
                        logging.info(f"éŸ³é¢‘åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {merged_path}")
                        
                        # å¤åˆ¶åˆ°æ ‡å‡†å®‰å…¨ä½ç½®ä»¥ç¡®ä¿å…¼å®¹æ€§
                        safe_path = SAFE_AUDIO_DIR / "Safe_Audio.wav"
                        shutil.copy2(merged_path, safe_path)
                        
                        # æ’­æ”¾åˆå¹¶åçš„å®Œæ•´éŸ³é¢‘
                        # æ³¨æ„ï¼šè¿™é‡Œä¸æ’­æ”¾å®Œæ•´éŸ³é¢‘ï¼Œå› ä¸ºå„æ®µå·²ç»æ’­æ”¾è¿‡äº†
                        # è¦æ’­æ”¾å¯ä»¥å–æ¶ˆä¸‹ä¸€è¡Œçš„æ³¨é‡Š
                        # self._play_audio_file(merged_path)
                        
                        return merged_path
                    except Exception as e:
                        logging.error(f"éŸ³é¢‘åˆå¹¶å¤±è´¥: {str(e)}")
                else:
                    logging.error("æ²¡æœ‰æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ®µè½ï¼Œæ— æ³•åˆå¹¶")
            except Exception as e:
                logging.error(f"SparkTTSåˆ†æ®µå¤„ç†å¼‚å¸¸: {str(e)}")
                import traceback
                logging.error(traceback.format_exc())
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        thread = threading.Thread(target=spark_tts_thread)
        thread.daemon = True
        thread.start()
        return thread

    def call_tts_api(self, text: str, language: str):
        """å¼‚æ­¥è°ƒç”¨TTS APIå¹¶æ’­æ”¾éŸ³é¢‘ï¼Œæ”¯æŒSparkTTSæ¨¡å¼"""
        if self.spark_tts:
            # ä½¿ç”¨æ”¹è¿›çš„åˆ†æ®µå¤„ç†
            thread = self._run_spark_tts(text)
            # ä¸ç­‰å¾…çº¿ç¨‹å®Œæˆï¼Œä¿æŒå¼‚æ­¥æ‰§è¡Œ
        else:
            def tts_thread():
                try:
                    url = "http://127.0.0.1:8010/tts"
                    payload = {"text": text, "language": language}
                    response = requests.post(url, json=payload, timeout=90)
                    if response.status_code == 200:
                        data = response.json()
                        output_file = data.get("output_file")
                        if (output_file):
                            self._play_audio_file(output_file)
                        else:
                            logging.error("TTS API è¿”å›æˆåŠŸä½†æ²¡æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„")
                    else:
                        logging.error(f"TTS API è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    logging.error(f"TTS APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            threading.Thread(target=tts_thread).start()

    # æ–°å¢ï¼š4ä½é‡åŒ–ä¼˜åŒ–å™¨ç¤ºä¾‹
    def _optimize_quant_4bit(self):
        """4ä½é‡åŒ–ä¼˜åŒ–"""
        logging.info("æ‰§è¡Œ4ä½é‡åŒ–ä¼˜åŒ–(é€šè¿‡ BitsAndBytesConfig å·²åœ¨åŠ è½½æ—¶åº”ç”¨)")

    # æ–°å¢ï¼šFlash Attentionä¼˜åŒ–å™¨ç¤ºä¾‹
    def _optimize_flash_attention(self):
        """Flash Attentionä¼˜åŒ–"""
        try:
            if importlib.util.find_spec("flash_attn") is not None:
                logging.info("æ‰§è¡ŒFlash Attentionä¼˜åŒ–(åœ¨åŠ è½½æ—¶å·²è®¾ç½® attn_implementation)")
            else:
                logging.warning("æœªæ£€æµ‹åˆ° flash_attn åº“ï¼Œæ— æ³•åº”ç”¨Flash Attentionä¼˜åŒ–")
        except Exception as e:
            logging.warning(f"Flash Attentionä¼˜åŒ–å¤±è´¥: {e}")

    def _format_error_with_type(self, error_type, detail, suggestion=None):
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œä½¿ç”¨å½©è‰²è¾“å‡º"""
        error_msg = f"{Fore.RED}é”™è¯¯ç±»å‹: {error_type}{Style.RESET_ALL}\n"
        error_msg += f"{Fore.RED}è¯¦ç»†ä¿¡æ¯: {detail}{Style.RESET_ALL}\n"
        if suggestion:
            error_msg += f"{Fore.MAGENTA}å»ºè®®: {suggestion}{Style.RESET_ALL}"
        return error_msg

    def _handle_loading_error(self, error, model_path):
        """å¤„ç†åŠ è½½é”™è¯¯å¹¶æä¾›æœ‰ç”¨çš„åé¦ˆ"""
        error_str = str(error)
        if "CUDA out of memory" in error_str:
            return self._format_error_with_type(
                "CUDAå†…å­˜ä¸è¶³",
                f"GPUå†…å­˜ä¸è¶³ä»¥åŠ è½½æ¨¡å‹ã€‚é”™è¯¯: {error_str}",
                "å°è¯•ä½¿ç”¨--load_in_4bité€‰é¡¹å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæˆ–ä½¿ç”¨--no_gpué€‰é¡¹åœ¨CPUä¸Šè¿è¡Œ"
            )
        elif "configuration_bitnet.py" in error_str:
            return self._format_error_with_type(
                "BitNeté…ç½®é”™è¯¯",
                f"æ‰¾ä¸åˆ°BitNeté…ç½®æ–‡ä»¶ã€‚é”™è¯¯: {error_str}",
                "è¯·è¿è¡Œ: pip install git+https://github.com/shumingma/transformers.git"
            )
        elif "No such file or directory" in error_str:
            return self._format_error_with_type(
                "æ–‡ä»¶è·¯å¾„é”™è¯¯",
                f"æ‰¾ä¸åˆ°æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model_path}. é”™è¯¯: {error_str}",
                "è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½å®Œæˆ"
            )
        else:
            return self._format_error_with_type(
                "æ¨¡å‹åŠ è½½å¤±è´¥",
                f"æœªçŸ¥é”™è¯¯: {error_str}",
                "è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼Œæˆ–å°è¯•é‡æ–°ä¸‹è½½æ¨¡å‹"
            )

    # BitNetä¸“ç”¨ä¼˜åŒ–å™¨
    def _optimize_bitnet(self):
        """é’ˆå¯¹BitNetæ¨¡å‹çš„ç‰¹æ®Šä¼˜åŒ–"""
        if self.model_type != "bitnet":
            logging.info(f"{Fore.YELLOW}éBitNetæ¨¡å‹ï¼Œè·³è¿‡BitNetä¼˜åŒ–{Style.RESET_ALL}")
            return

        logging.info(f"{Fore.CYAN}å¼€å§‹BitNetæ¨¡å‹ä¼˜åŒ–...{Style.RESET_ALL}")
        try:
            # å¯¼å…¥BitNetæ¨¡å—
            try:
                from transformers.models.bitnet import modeling_bitnet
                logging.info(f"{Fore.GREEN}æˆåŠŸå¯¼å…¥BitNetæ¨¡å—{Style.RESET_ALL}")
                have_bitnet = True
            except ImportError:
                logging.warning(f"{Fore.YELLOW}å¯¼å…¥BitNetæ¨¡å—å¤±è´¥ï¼Œéœ€è¦å®‰è£…ç‰¹æ®Šç‰ˆæœ¬çš„transformers{Style.RESET_ALL}")
                logging.warning("è¯·è¿è¡Œ: pip install git+https://github.com/shumingma/transformers.git")
                have_bitnet = False
                return

            # æ£€æŸ¥é‡åŒ–é…ç½®
            if hasattr(self.model, 'config') and hasattr(self.model.config, 'quantization_config'):
                quant_config = self.model.config.quantization_config
                logging.info(f"BitNeté‡åŒ–é…ç½®: {quant_config}")
            
            # æ£€æµ‹BitLinearå±‚
            bitlinear_count = 0
            for name, module in self.model.named_modules():
                if "BitLinear" in str(type(module)):
                    bitlinear_count += 1
                    if bitlinear_count <= 3:  # åªæ‰“å°å‰å‡ ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                        logging.info(f"å‘ç°BitLinearå±‚: {name}")
            
            if bitlinear_count > 0:
                logging.info(f"{Fore.GREEN}å…±å‘ç°{bitlinear_count}ä¸ªBitLinearå±‚{Style.RESET_ALL}")
            else:
                logging.warning(f"{Fore.YELLOW}æœªæ£€æµ‹åˆ°BitLinearå±‚ï¼Œè¯·ç¡®è®¤æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½{Style.RESET_ALL}")
            
            # å¯ä»¥åœ¨æ­¤æ·»åŠ BitNetç‰¹å®šçš„ä¼˜åŒ–é€»è¾‘
            # ...

            logging.info(f"{Fore.GREEN}BitNetæ¨¡å‹ä¼˜åŒ–å®Œæˆ{Style.RESET_ALL}")
        except Exception as e:
            logging.error(f"{Fore.RED}BitNetä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {str(e)}{Style.RESET_ALL}")
            import traceback
            logging.debug(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")

# pydubç›´æ¥æ’­æ”¾è¾…åŠ©å‡½æ•°ï¼Œé˜²æ­¢ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
def _direct_play_pydub(seg, path):
    """ä½¿ç”¨pydubç›´æ¥æ’­æ”¾éŸ³é¢‘ï¼Œé¿å…åˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
    try:
        import wave
        import numpy as np
        import sounddevice as sd
        
        # ç›´æ¥ä»æ–‡ä»¶è¯»å–
        with wave.open(str(path), 'rb') as wf:
            # è·å–éŸ³é¢‘å‚æ•°
            channels = wf.getnchannels()
            sample_width = wf.getsampwidth()
            sample_rate = wf.getframerate()
            frames = wf.readframes(wf.getnframes())
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            dtype = {1: np.int8, 2: np.int16, 4: np.int32}.get(sample_width, np.int16)
            audio_data = np.frombuffer(frames, dtype=dtype)
            
            # é‡å¡‘æ•°ç»„ä»¥åŒ¹é…é€šé“æ•°
            if channels > 1:
                audio_data = audio_data.reshape(-1, channels)
            
            # æ’­æ”¾éŸ³é¢‘
            sd.play(audio_data, sample_rate)
            sd.wait()  # ç­‰å¾…æ’­æ”¾å®Œæˆ
            
        return True
    except Exception as e:
        logging.error(f"ç›´æ¥æ’­æ”¾å¤±è´¥: {str(e)}")
        # å›é€€åˆ°åŸå§‹æ–¹æ³•
        play(seg)
        return False

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description=f"{Fore.CYAN}Reverie AI{Style.RESET_ALL} - å¤šæ¶æ„LLMæ¨ç†å¼•æ“")
    parser.add_argument("--model_path", type=str, required=True,
                        help="æ¨¡å‹è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰")
    parser.add_argument("--use_gpu", action="store_true",
                        help="å¯ç”¨GPUåŠ é€Ÿ")
    parser.add_argument("--load_in_4bit", action="store_true",
                        help="ä½¿ç”¨4ä½é‡åŒ–ï¼ˆéœ€è¦CUDAï¼‰")
    parser.add_argument("--use_tts", action="store_true",
                        help="å¯ç”¨TTSåŠŸèƒ½")
    parser.add_argument("--SparkTTS", action="store_true",
                        help="å¯ç”¨SparkTTSæ¨¡å¼")
    parser.add_argument("--prompt_text", type=str,
                        default="é›¨æ—é‡Œå¯ä»¥äº¤ç»™æˆ‘çš„çœ·å±ä»¬ï¼ŒåŸå¸‚é‡Œæˆ‘å°±æ‹œæ‰˜ä¸€äº›å°å­©å­å§ã€‚",
                        help="SparkTTSæç¤ºæ–‡æœ¬")
    parser.add_argument("--prompt_speech_path", type=str,
                        default="models/tts/Nahida.wav",
                        help="SparkTTSå‚è€ƒéŸ³é¢‘è·¯å¾„")
    parser.add_argument("--device", type=str, default="0",
                        help="GPUè®¾å¤‡ID")
    parser.add_argument("--save_dir", type=str, default="Audio",
                        help="éŸ³é¢‘ä¿å­˜ç›®å½•")
    parser.add_argument("--model_dir", type=str,
                        help="SparkTTSæ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--text", type=str,
                        help="å¾…ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¦‚æœæŒ‡å®šåˆ™éäº¤äº’æ¨¡å¼")
    args = parser.parse_args()
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        parser.error(f"{Fore.RED}æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}{Style.RESET_ALL}")
    
    return args

def main():
    try:
        args = parse_args()
        # æ³¨æ„: ä¸å†è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ python clean.py æˆ–è¿è¡Œ start-clean.bat
        
        # éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        model_path = args.model_path
        if not os.path.exists(model_path):
            print(f"{Fore.RED}é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}{Style.RESET_ALL}")
            sys.exit(1)
            
        # æ£€æŸ¥æ¨¡å‹ç›®å½•å†…å®¹
        try:
            if os.path.isdir(model_path):
                dir_contents = os.listdir(model_path)
                print(f"{Fore.BLUE}æ¨¡å‹ç›®å½•å†…å®¹: {dir_contents[:5]}{Style.RESET_ALL}" + ("..." if len(dir_contents) > 5 else ""))
                
                # æ£€æŸ¥æ˜¯å¦æœ‰config.jsonæ–‡ä»¶
                if "config.json" in dir_contents:
                    print(f"{Fore.GREEN}æ‰¾åˆ°config.jsonæ–‡ä»¶{Style.RESET_ALL}")
                else:
                    print(f"{Fore.YELLOW}è­¦å‘Š: æœªæ‰¾åˆ°config.jsonæ–‡ä»¶{Style.RESET_ALL}")
                    
                # æ£€æŸ¥æ˜¯å¦æœ‰tokenizeræ–‡ä»¶
                if any(f.startswith("tokenizer") for f in dir_contents):
                    print(f"{Fore.GREEN}æ‰¾åˆ°tokenizeræ–‡ä»¶{Style.RESET_ALL}")
                else:
                    print(f"{Fore.YELLOW}è­¦å‘Š: æœªæ‰¾åˆ°tokenizeræ–‡ä»¶{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.RED}æ£€æŸ¥æ¨¡å‹ç›®å½•æ—¶å‡ºé”™: {str(e)}{Style.RESET_ALL}")
        
        reverie = ReverieAI(
            args.model_path,
            use_gpu=args.use_gpu,
            load_in_4bit=args.load_in_4bit,
            use_tts=args.use_tts,
            spark_tts=args.SparkTTS,
            prompt_text=args.prompt_text,
            prompt_speech_path=args.prompt_speech_path,
            device=args.device,
            save_dir=args.save_dir,
            model_dir=args.model_dir
        )
        logging.info("æ¨¡å‹åŠ è½½å®Œæˆï¼Œç­‰å¾…è¾“å…¥...")
        print(f"{Fore.GREEN}MODEL_READY{Style.RESET_ALL}", flush=True) 

        # éäº¤äº’æ¨¡å¼ï¼šå¦‚æœä¼ å…¥ --text å‚æ•°åˆ™ç›´æ¥ç”Ÿæˆå›å¤åé€€å‡º
        if args.text:
            user_input = args.text.strip()
            start_time = time.time()
            response_generator = reverie.generate_response(user_input)
            full_response = []
            for token in response_generator:
                if token == "||END||":
                    break
                if token:
                    print(token, end='', flush=True)
                    full_response.append(token)
            complete_response = "".join(full_response).strip()
            token_count = 0
            try:
                if reverie.model_loaded:
                    if reverie.model_type == "gguf":
                        token_count = len(reverie.llm.tokenize(complete_response.encode()))
                    else:
                        token_count = len(reverie.tokenizer.encode(
                            complete_response, 
                            add_special_tokens=False
                        ))
            except Exception as e:
                logging.error(f"Tokenè®¡æ•°é”™è¯¯: {str(e)}")
            print(f"\n\n{Fore.CYAN}[è€—æ—¶ {time.time()-start_time:.2f}s | ç”ŸæˆTokenæ•°: {token_count}]{Style.RESET_ALL}\n", flush=True)
            if complete_response and reverie.use_tts:
                lang = reverie._detect_language(complete_response)
                logging.info(f"æ–‡æœ¬ç”Ÿæˆå®Œæˆï¼Œè°ƒç”¨TTSï¼Œè¯­è¨€: {lang}")
                reverie.call_tts_api(complete_response, lang)
            return

        # äº¤äº’å¾ªç¯æ¨¡å¼
        while True:
            try:
                user_input = input("> ").strip()
                if user_input.lower() in ["exit", "quit"]:
                    break
                
                # æ·»åŠ æ¸…é™¤å¯¹è¯å†å²çš„å‘½ä»¤
                if user_input.lower() in ["clear", "reset", "æ¸…é™¤", "é‡ç½®"]:
                    reverie.clear_conversation_history()
                    print(f"{Fore.CYAN}å·²æ¸…é™¤å¯¹è¯å†å²ã€‚{Style.RESET_ALL}")
                    continue
                
                start_time = time.time()
                response_generator = reverie.generate_response(user_input)
                full_response = []
                for token in response_generator:
                    if token == "||END||":
                        break
                    if token:
                        print(token, end='', flush=True)
                        full_response.append(token)
                complete_response = "".join(full_response).strip()
                token_count = 0
                try:
                    if reverie.model_loaded:
                        if reverie.model_type == "gguf":
                            token_count = len(reverie.llm.tokenize(complete_response.encode()))
                        else:
                            token_count = len(reverie.tokenizer.encode(
                                complete_response, 
                                add_special_tokens=False
                            ))
                except Exception as e:
                    logging.error(f"Tokenè®¡æ•°é”™è¯¯: {str(e)}")
                print(f"\n\n{Fore.CYAN}[è€—æ—¶ {time.time()-start_time:.2f}s | ç”ŸæˆTokenæ•°: {token_count}]{Style.RESET_ALL}\n", flush=True)
                if complete_response and reverie.use_tts:
                    lang = reverie._detect_language(complete_response)
                    logging.info(f"æ–‡æœ¬ç”Ÿæˆå®Œæˆï¼Œè°ƒç”¨TTSï¼Œè¯­è¨€: {lang}")
                    reverie.call_tts_api(complete_response, lang)
            except KeyboardInterrupt:
                logging.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                break
    except Exception as e:
        error_msg = [
            f"{Fore.RED}âš ï¸ ä¸¥é‡é”™è¯¯ï¼{Style.RESET_ALL}",
            f"{Fore.RED}é”™è¯¯ç±»å‹: {type(e).__name__}{Style.RESET_ALL}",
            f"{Fore.RED}è¯¦ç»†ä¿¡æ¯: {str(e)}{Style.RESET_ALL}",
            f"{Fore.YELLOW}\nè¿½è¸ªä¿¡æ¯:{Style.RESET_ALL}",
            *traceback.format_tb(e.__traceback__)
        ]
        print("\n".join(error_msg), file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()